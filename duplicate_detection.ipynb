{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.8 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Duplicate detected.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import csv\n",
    "\n",
    "# Detect Duplicate\n",
    "def duplicateDetection(df, column):\n",
    "    duplicate = df[df.duplicated(column)==1][column] # Find duplicate rows\n",
    "    dup = df[df[column].isin(duplicate)].sort_values(column) # Find rows with duplicate information\n",
    "    dup[column+'_dup']=True # Label which fields they are duplicated on\n",
    "    return dup\n",
    "\n",
    "def main():\n",
    "    df_list = [pd.DataFrame() for i in range(len(interestingCol))] # Create empty dataframes\n",
    "    for i in range(len(interestingCol)):\n",
    "        df_list[i]=duplicateDetection(data_scientist_duplicate_detection, interestingCol[i]) # Detect duplicates on each field\n",
    "    result = df_list[0].append(df_list[1:]) # Append them to a single dataframe \n",
    "    result.to_csv('duplicates.csv',index=None)\n",
    "\n",
    "\n",
    "    # # Or use the code below, since there can't be too many interesting columns\n",
    "    # #\n",
    "    # id = duplicateDetection(data_scientist_duplicate_detection,'id')\n",
    "    # name = duplicateDetection(data_scientist_duplicate_detection, 'name')\n",
    "    # id.append(name)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_scientist_duplicate_detection = pd.read_csv('data_scientist_duplicate_detection.csv')\n",
    "    interestingCol = ('id','name') # Only id and name are interesting columns, because they are the keys of a company\n",
    "    main()\n",
    "    print('Duplicate detected.')\n"
   ]
  }
 ]
}